{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "import TransFTrain as train\n",
    "import TransFTrain.nn as nn\n",
    "from TransFTrain.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import struct, gzip\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "batch_size = 500\n",
    "hidden_dim=100\n",
    "lr =0.001\n",
    "weight_decay=0.0001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_filename: str,\n",
    "        label_filename: str,\n",
    "        transforms: Optional[List] = None,\n",
    "    ):  \n",
    "        self.images, self.labels = self.parse_mnist(image_filename, label_filename)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index) -> object:\n",
    "        data= self.images[index]\n",
    "        label= self.labels[index]\n",
    "        if isinstance(index, int):\n",
    "            data = self.apply_transforms(data.reshape(28, 28, 1))\n",
    "        else:\n",
    "            data = np.array([self.apply_transforms(i.reshape(28, 28, 1)) for i in data])\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def parse_mnist(self, image_filename, label_filename):\n",
    "        image_file = gzip.open(image_filename, 'rb')\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", image_file.read(16))\n",
    "        image_data = image_file.read()\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8)\n",
    "        images = (images.reshape(num_images, rows * cols)/255).astype(np.float32)\n",
    "        label_file = gzip.open(label_filename, 'rb')\n",
    "        magic, num_labels = struct.unpack(\">II\", label_file.read(8))\n",
    "        label_data = label_file.read()\n",
    "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
    "        image_file.close()\n",
    "        label_file.close()\n",
    "        return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, norm=nn.BatchNorm1d, drop_prob=0.1):\n",
    "        super(ResidualBlock).__init__()\n",
    "        self.linear1 = nn.Linear(dim, hidden_dim)\n",
    "        self.norm1 = norm(hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "        self.linear2 = nn.Linear(hidden_dim, dim)\n",
    "        self.norm2 = norm(dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = x\n",
    "        x = self.linear1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = x+ret\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "class MLPResNet(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=100, num_blocks=3, num_classes=10, norm=nn.BatchNorm1d, drop_prob=0.1):\n",
    "        super(MLPResNet).__init__()\n",
    "        self.linear1 = nn.Linear(dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.module_list = self._model_list(dim, hidden_dim, num_blocks, num_classes, norm, drop_prob)\n",
    "\n",
    "    def _model_list(self, dim, hidden_dim, num_blocks, num_classes, norm, drop_prob):\n",
    "        module_list = []\n",
    "        for _ in range(num_blocks):\n",
    "            module_list.append(ResidualBlock(hidden_dim, hidden_dim//2, norm=norm, drop_prob=drop_prob))\n",
    "        module_list.append(nn.Linear(hidden_dim, num_classes))\n",
    "        return nn.Sequential(*module_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.module_list(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =train.data.MNISTDataset(data_dir+\"/train-images-idx3-ubyte.gz\",\n",
    "                                        data_dir+\"/train-labels-idx1-ubyte.gz\")\n",
    "train_dataloader = train.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "test_dataset = train.data.MNISTDataset(data_dir+\"/t10k-images-idx3-ubyte.gz\",\n",
    "                                        data_dir+\"/t10k-labels-idx1-ubyte.gz\")\n",
    "test_dataloader = train.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPResNet(784, hidden_dim)\n",
    "model.train()\n",
    "opt = train.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] train average error rate 0.8820166666666667\n",
      "Epoch[0] train average loss 3.8040648182233174\n",
      "Epoch[1] train average error rate 0.8806333333333334\n",
      "Epoch[1] train average loss 3.794713364044825\n",
      "Epoch[2] train average error rate 0.8819\n",
      "Epoch[2] train average loss 3.8052514215310413\n",
      "Epoch[3] train average error rate 0.8820833333333333\n",
      "Epoch[3] train average loss 3.8031845609347026\n",
      "Epoch[4] train average error rate 0.8829666666666667\n",
      "Epoch[4] train average loss 3.8021618644396464\n",
      "Epoch[5] train average error rate 0.8828666666666667\n",
      "Epoch[5] train average loss 3.797142763932546\n",
      "Epoch[6] train average error rate 0.8824666666666666\n",
      "Epoch[6] train average loss 3.799217963218689\n",
      "Epoch[7] train average error rate 0.8812166666666666\n",
      "Epoch[7] train average loss 3.801325782140096\n",
      "Epoch[8] train average error rate 0.8813833333333333\n",
      "Epoch[8] train average loss 3.7962018648783364\n",
      "Epoch[9] train average error rate 0.882\n",
      "Epoch[9] train average loss 3.8013573944568635\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.SoftmaxLoss()\n",
    "for i in range(epochs):\n",
    "    correct_sum = 0\n",
    "    loss_sum = 0.0\n",
    "    batches = 0\n",
    "    for image,lable in train_dataloader:\n",
    "        batches+=1\n",
    "        opt.reset_grad()\n",
    "        image =image.reshape((-1,784))\n",
    "        predict = model(image)\n",
    "        loss = loss_func(predict, lable)\n",
    "        correct_sum += (predict.numpy().argmax(1) == lable.numpy()).sum()\n",
    "        loss.backward()\n",
    "        loss_sum += loss.numpy()\n",
    "    print(f\"Epoch[{i}] train average error rate\", 1-correct_sum/len(train_dataloader.dataset))\n",
    "    print(f\"Epoch[{i}] train average loss\", loss_sum/batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9] test average error rate 0.8627\n",
      "Epoch[9] test average loss 3.765920538561685\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for image,lable in test_dataloader:\n",
    "    batches+=1\n",
    "    image =image.reshape((-1,784))\n",
    "    predict = model(image)\n",
    "    loss = loss_func(predict, lable)\n",
    "    correct_sum += (predict.numpy().argmax(1) == lable.numpy()).sum()\n",
    "    loss_sum += loss.numpy()\n",
    "print(f\"Epoch[{i}] test average error rate\", 1-correct_sum/len(train_dataloader.dataset))\n",
    "print(f\"Epoch[{i}] test average loss\", loss_sum/batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transftrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce8aa7df9b28ec054582940e45896bd49a5182198d94d9d331409c0c629807d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
